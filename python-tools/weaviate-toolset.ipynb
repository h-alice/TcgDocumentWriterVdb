{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3555aa34",
   "metadata": {},
   "source": [
    "# Data Ingestion for Document Retrieval Service\n",
    "\n",
    "This notebook provides a comprehensive toolkit for ingesting data into our Weaviate vector database. This process is a fundamental part of our document retrieval service, as it prepares and populates the database with the information required for efficient and accurate searching.\n",
    "\n",
    "For operational use, the toolkit presented here can be packaged into a standalone script to facilitate automated data ingestion workflows. (in other words, pack the notebook into script if you want!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import requests\n",
    "import weaviate\n",
    "from weaviate import classes as wvc\n",
    "from weaviate.classes.query import MetadataQuery, QueryReference\n",
    "from weaviate.classes.config import Property, DataType, ReferenceProperty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a062c5d",
   "metadata": {},
   "source": [
    "## Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef00458",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedDoc = namedtuple(\"ProcessedDoc\", [\"docType\", \"docSummary\", \"docContent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd6b27",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_UPPER_LIMIT = 512 - 15 # 15 tokens for special tokens\n",
    "\n",
    "# Endpoint for embedding service, related to Weaviate.\n",
    "#\n",
    "# IMPORTANT NOTE: A Weaviate collection is bond to the specified embedding service during creation.\n",
    "# If you want to change the embedding service after creation, you need to recreate the collection.\n",
    "#\n",
    "# This is the endpoint for the **embedding service for Weaviate** (not your own embedding service!),\n",
    "# which should be running in the same Docker network as Weaviate, and have the same hostname as docker-compose defined.\n",
    "EMBEDDING_ENDPOINT = \"http://embedding:8080\"\n",
    "\n",
    "MIN_CHUNK_SIZE = 8 # Minimum chunk size for document processing, smaller will be DISCARDED.\n",
    "MAX_RERANKER_SIZE = 8192 # Max size for reranker service, an article more than this will be DISCARDED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b005de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_COLLECTION_ORIG_NAME = \"%sOrig\" # Name formatter for original document collection\n",
    "DOCUMENT_CLASS_MAP = {\n",
    "    \"簽\": \"Qian\",\n",
    "    \"函\": \"Han\",\n",
    "    \"陳情\": \"Complaint\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8323170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate instance\n",
    "client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f310f",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b869e6",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def slicer(document: str, chunk_length: int, overlap: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Slices a document into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        document: The input string to be sliced.\n",
    "        chunk_length: The desired length of each chunk.\n",
    "        overlap: The number of characters to overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, where each string is a chunk of the original document.\n",
    "    \"\"\"\n",
    "    if not isinstance(document, str):\n",
    "        raise TypeError(\"Input 'document' must be a string.\")\n",
    "    if not isinstance(chunk_length, int) or chunk_length <= 0:\n",
    "        raise ValueError(\"'chunk_length' must be a positive integer.\")\n",
    "    if not isinstance(overlap, int) or overlap < 0:\n",
    "        raise ValueError(\"'overlap' must be a non-negative integer.\")\n",
    "    if overlap >= chunk_length:\n",
    "        raise ValueError(\"'overlap' cannot be greater than or equal to 'chunk_length'.\")\n",
    "\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    doc_len = len(document)\n",
    "\n",
    "    while start_index < doc_len:\n",
    "        end_index = start_index + chunk_length\n",
    "        chunk = document[start_index:end_index]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Move the start index for the next chunk\n",
    "        # If the next chunk would exceed the document length, we stop\n",
    "        if start_index + chunk_length - overlap >= doc_len and start_index + chunk_length >= doc_len :\n",
    "            break\n",
    "        start_index += (chunk_length - overlap)\n",
    "        # Ensure the last chunk doesn't go beyond the document length\n",
    "        # if start_index + chunk_length > doc_len and start_index < doc_len :\n",
    "        #   chunks.append(document[start_index:])\n",
    "        #   break\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3c7e5",
   "metadata": {},
   "source": [
    "## Getting Document Data\n",
    "\n",
    "This is just an example, you should implement your own logic to prepare the data.\n",
    "\n",
    "The system requires a document paired with summary to provide a document matching process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03568b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "doc_db = list[ProcessedDoc]\n",
    "\n",
    "with open(\"../resources/processed_documents.pkl\", \"rb\") as f:\n",
    "    doc_db: list[ProcessedDoc] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff6f28",
   "metadata": {},
   "source": [
    "## Querying Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DOCUMENT_CLASS = \"陳情\" # NOTE: Try other document class to see how it works!\n",
    "user_query = \"網路訊號不佳\"\n",
    "\n",
    "\n",
    "TARGET_COLLECTION = DOCUMENT_CLASS_MAP[TARGET_DOCUMENT_CLASS] # No need to modify this line.\n",
    "\n",
    "document_collection = client.collections.get(TARGET_COLLECTION)   \n",
    "\n",
    "vd_query_response = document_collection.query.hybrid(\n",
    "    query=user_query,\n",
    "    query_properties=[\"content\"],\n",
    "    alpha=0.5,\n",
    "    limit=3,\n",
    "    return_metadata=MetadataQuery(score=True, explain_score=True),\n",
    "    return_references=QueryReference(\n",
    "        link_on=\"orig\",\n",
    "        return_properties=[\"content\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Query: {user_query}\")\n",
    "print(f\"Collection: {TARGET_DOCUMENT_CLASS}\")\n",
    "\n",
    "for obj in vd_query_response.objects:\n",
    "    print(f\"Document Chunk: \\n{obj.properties['content']}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Original Document: \\n{obj.references['orig']._CrossReference__objects[0].properties['content']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614aad4f",
   "metadata": {},
   "source": [
    "### GraphQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL payload, with a placeholder for the user query\n",
    "# The query logic is exactly the same as the one used in actual Retrieval service.\n",
    "graphql_payload = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    %s( # Placeholder for document chunk collection\n",
    "      hybrid: {\n",
    "        query: \"%s\",         # Your user query\n",
    "        alpha: 0.5,          # Balance between vector/keyword search\n",
    "        properties: [\"content\"] # Properties for keyword (BM25) search\n",
    "      },\n",
    "      limit: 2 # Limit the number of results FROM Weaviate\n",
    "    ) {\n",
    "      # --- Specify properties you need for reranking or display ---\n",
    "      content   # You specifically extracted this for the reranker\n",
    "\n",
    "      # --- Add any other properties if needed ---\n",
    "      # Example: other_property\n",
    "\n",
    "      # --- Request metadata ---\n",
    "      _additional {\n",
    "        score        # The hybrid search score, needed for sorting\n",
    "        explainScore # Breakdown of keyword/vector contribution\n",
    "        id           # Useful for unique identification\n",
    "      }\n",
    "      orig {\n",
    "        ... on %sOrig { # Placeholder for original document collection\n",
    "          content\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"TaipeiFree訊號很差\"\n",
    "\n",
    "response = requests.post(\n",
    "    url=\"http://localhost:8080/v1/graphql\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    json={\n",
    "        # NOTE: The query formatting process is same as the one used in actual Retrieval service.\n",
    "        \"query\": graphql_payload % (TARGET_COLLECTION, user_query, TARGET_COLLECTION),\n",
    "        \"variables\": None,\n",
    "        \"operationName\": None\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de7194",
   "metadata": {},
   "source": [
    "## ⚠️ DANGEROUS ⚠️ Define Document Collection\n",
    "\n",
    "**WARNING**: This procedure will drop the existing collection and create a new one. All data will be lost. Do **NOT** run this if you just want to insert new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e49184",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DOCUMENT_CLASS = \"陳情\" # ⚠️WARN: Modify as needed\n",
    "\n",
    "# Constants that need not be modified\n",
    "COLLECTION = DOCUMENT_CLASS_MAP[TARGET_DOCUMENT_CLASS] \n",
    "ORIGINAL_DOCUMENT_CLASS = DOCUMENT_COLLECTION_ORIG_NAME % COLLECTION\n",
    "\n",
    "# We use two-way references to link the original documents to their chunks.\n",
    "#\n",
    "# Collection for the original documents\n",
    "# Remove existing class if it exists\n",
    "client.collections.delete(COLLECTION)\n",
    "client.collections.delete(ORIGINAL_DOCUMENT_CLASS)\n",
    "\n",
    "# Make a collection for the document class.\n",
    "documentsOrig = client.collections.create(\n",
    "    name=ORIGINAL_DOCUMENT_CLASS,\n",
    "    properties=[\n",
    "        Property(name=\"content\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n",
    ")\n",
    "\n",
    "documents = client.collections.create(\n",
    "    name=COLLECTION,\n",
    "    properties=[\n",
    "        Property(name=\"content\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_openai(\n",
    "        base_url=EMBEDDING_ENDPOINT,  # The URL of the embedding service\n",
    "    ),\n",
    "    references=[\n",
    "        ReferenceProperty(\n",
    "            name=\"orig\",\n",
    "            target_collection=ORIGINAL_DOCUMENT_CLASS,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9b25e",
   "metadata": {},
   "source": [
    "### Add Data to Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the test data to the collection\n",
    "documentOrig = client.collections.get(ORIGINAL_DOCUMENT_CLASS)\n",
    "chunkCollection = client.collections.get(COLLECTION)\n",
    "\n",
    "selected_docs = [ x for x in doc_db if x.docType == TARGET_DOCUMENT_CLASS ]\n",
    "\n",
    "for i, doc in enumerate(selected_docs):\n",
    "    chunkSize = EMBEDDING_UPPER_LIMIT\n",
    "    overlap = 150 # You can try different overlaps.\n",
    "\n",
    "    # Insert original document\n",
    "    docId = documentOrig.data.insert(\n",
    "        properties={\n",
    "            \"content\": doc.docContent, # Original document\n",
    "        },\n",
    "    )\n",
    "    print(f\"Document #{i}: Original document inserted.\")\n",
    "\n",
    "    chunksSum = slicer(doc.docSummary, chunk_length=chunkSize, overlap=overlap) # Slice the summary into chunks\n",
    "\n",
    "    # Process summary chunk\n",
    "    for j, chunk in enumerate(chunksSum):\n",
    "        chunkCollection.data.insert(\n",
    "            properties={\n",
    "                \"content\": chunk,\n",
    "            },\n",
    "            references={\"orig\": docId} # UUID reference to the original document\n",
    "        )\n",
    "        print(f\"Document #{i}: Summary Chunk #{j} inserted.\")\n",
    "    \n",
    "    \n",
    "    chunksOrig = slicer(doc.docContent, chunk_length=chunkSize, overlap=overlap) # You can try different overlaps.\n",
    "    \n",
    "\n",
    "    # Process document chunk\n",
    "    for j, chunk in enumerate(chunksOrig):\n",
    "        chunkCollection.data.insert(\n",
    "            properties={\n",
    "                \"content\": chunk,\n",
    "            },\n",
    "            references={\"orig\": docId} # UUID reference to the original document\n",
    "        )\n",
    "        print(f\"Document #{i}: Chunk #{j} inserted.\")\n",
    "\n",
    "    print(f\"Document #{i}: Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e9c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
